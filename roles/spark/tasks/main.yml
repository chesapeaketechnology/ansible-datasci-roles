---
- name: Validate Spark Installed
  stat:
    path: "{{ stat_file }}"
  register: spark_install_query

- name: Exit Play
  meta: end_play
  when: spark_install_query.stat.exists == True

- name: Create service account for Spark
  user: name={{ spark.user }}
    system=yes
    shell={{ spark.user_shell }}
    state=present
    groups="{{ spark.user_groups | join(',') }}"

- name: set spark fact
  set_fact: spark_installed=True

- name: set spark installation path fact
  set_fact: spark_installation_dir=spark-{{ spark.version }}-bin-{{ spark.hadoop_version }}

- name: set spark archive fact
  set_fact: spark_archive=spark-{{ spark.version }}-bin-{{ spark.hadoop_version }}.tgz

- name: set spark download location fact
  set_fact: spark_download={{ spark.download_location }}/spark-{{ spark.version }}/{{ spark_archive }}

- name: define number of spark workers
  set_fact: number_of_workers="{{ groups[target_server_group] | length | int }}"

- name: define installation directories
  set_fact: install_dir="{{ spark.install_dir }}" install_temp_dir="{{ spark.install_temp_dir }}"

- name: create install directory
  file:
    path: "{{ install_dir }}/{{ spark_installation_dir }}"
    state: directory

- name: create install temp directory
  file:
    path: "{{ install_temp_dir }}"
    state: directory

- name: create event log directories
  file:
    path: "{{ spark.eventlog_dir }}"
    state: directory
    mode: "1777"

- name: stop spark
  shell: "sbin/stop-all.sh"
  args:
    chdir: "{{ install_dir}}/{{ spark_installation_dir }}"
  ignore_errors: yes
  when: inventory_hostname in groups[target_server_group]

- debug:
    msg: "Downloading Spark from: {{ spark_download }}"

- name: download spark
  get_url: url="{{ spark_download }}" dest="{{ install_temp_dir }}/{{ spark_archive }}"

- name: unarchive to the install directory
  unarchive: copy=no dest={{ install_dir }} src={{ install_temp_dir }}/{{ spark_archive }}
  become: true

- name: create spark working directory
  file:
    path: "{{ spark.working_dir }}"
    state: directory

- name: set spark-env.sh
  template: src="spark-env-sh.j2" dest="{{ install_dir }}/{{ spark_installation_dir }}/conf/spark-env.sh"

- name: Read Java home
  shell: dirname $(dirname $(readlink $(readlink $(which java))))
  register: java_home_reg

- name: Update java_home
  set_fact: "java_home={{ java_home_reg.stdout_lines[0] }}"

- name: Export JAVA_HOME within spark env
  lineinfile:
    dest: "{{ install_dir }}/{{ spark_installation_dir }}/conf/spark-env.sh"
    regexp: "^export JAVA_HOME="
    line: "export JAVA_HOME={{ java_home }}"

- name: set spark-defaults.conf
  template: src="spark-defaults-conf.j2" dest="{{ install_dir }}/{{ spark_installation_dir }}/conf/spark-defaults.conf"

- name: set slaves
  template: src="slaves.j2" dest="{{ install_dir }}/{{ spark_installation_dir }}/conf/slaves"

- name: "Setup JDBC driver for PosgreSQL"
  maven_artifact:
    repository_url: https://repo1.maven.org/maven2/
    group_id: org.postgresql
    artifact_id: postgresql
    dest: "{{ install_dir }}/{{ spark_installation_dir }}/jars/"
    mode: 0644

# Environment setup.
- name: add spark profile to startup
  template:
    src: spark-profile.sh.j2
    dest: /etc/profile.d/spark-profile.sh
    mode: 0644

- name: Drop successful install note
  file:
    path: "{{ stat_file }}"
    state: touch
