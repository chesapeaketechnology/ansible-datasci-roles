---
- name: Create hadoop Group
  group:
    name: "{{ hadoop_group }}"
    state: present

- name: Create hadoop User
  user:
    name: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    shell: /bin/bash

- name: Add SSH authorized key
  authorized_key:
    user: "{{ hadoop_user }}"
    key: "{{ lookup('file', ssh_public_key) }}"

- name: Copy private key into place
  template:
    src: "{{ ssh_private_key }}"
    dest: "/home/{{ hadoop_user }}/.ssh/id_rsa"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group}}"
    mode: 0600

- name: Copy private key into place as hadoop_rsa
  template:
    src: "{{ ssh_private_key }}" #"~/.ssh/id_rsa"
    dest: "/home/{{ hadoop_user }}/.ssh/hadoop_rsa"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group}}"
    mode: 0600

- name: Create temporary directory
  file:
    path: "{{ hadoop_temp }}"
    state: directory

- name: Download hadoop binary package archive
  get_url:
    url: "{{ hadoop_url }}"
    dest: "{{ hadoop_bin_tmp }}"
    timeout: 600

# - name: Download the corresponding cryptographic signature file
#   get_url:
#     url: "{{ hadoop_sig_url }}"
#     dest: "{{ hadoop_sig_tmp }}"
#     timeout: 600

# - name: Download the hadoop PGP keys
#   get_url:
#     url: "{{ hadoop_keys_url }}"
#     dest: "{{ hadoop_keys_tmp }}"
#     timeout: 600
#   changed_when: False

# - name: Import hadoop PGP keys
#   shell: gpg --import {{ hadoop_keys_tmp }}
#   changed_when: False

# - name: Verify hadoop binary package archive authenticity
#   shell: gpg --verify {{ hadoop_sig_tmp }} {{ hadoop_bin_tmp }}
#   changed_when: False

- name: Extract downloaded hadoop archive
  unarchive:
    copy: no
    creates: /usr/local/hadoop-{{ hadoop_version }}
    dest: /usr/local
    src: "{{ hadoop_bin_tmp }}"
    owner: "{{ hadoop_user}}"
    group: "{{ hadoop_group }}"
  become: true

#idempotent command
- name: rename hadoop
  command:
    cmd: "mv {{ hadoop_home }}-{{ hadoop_version }} {{ hadoop_home }}"
    creates: "{{ hadoop_home }}"
    removes: "{{ hadoop_home }}-{{ hadoop_version }}"

# Environment setup.
- name: add hadoop profile to startup
  template:
    src: hadoop-profile.sh.j2
    dest: /etc/profile.d/hadoop-profile.sh
    mode: 0644

- name: export env variable HADOOP_SSH_OPTS
  lineinfile:
    dest: "/home/{{ hadoop_user}}/.bashrc"
    regexp: "HADOOP_SSH_OPTS="
    line: 'export HADOOP_SSH_OPTS="-i /home/{{ hadoop_user }}/.ssh/hadoop_rsa"'

# Idempotent way to build a /etc/hosts file with Ansible using your Ansible hosts inventory for a source.
# Will include all hosts the playbook is run on.
# Inspired from http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html

- name: Build hosts file
  lineinfile:
    dest: /etc/hosts
    regexp: "{{ hostvars[item]['private_ipv4_addresses'][0] }}"
    line: "{{ hostvars[item]['private_ipv4_addresses'][0] }} {{ item }}"
    state: present
  with_items:
    - "{{ groups[target_server_group] }}"

  #with_together:
  #- "{{ private_ips }}"
  #- "{{ fqdns }}"
- name: remove localhost from hosts
  lineinfile:
    dest: /etc/hosts
    regexp: "127.0.1.1"
    state: absent

- name: Create HDFS directories
  file:
    path: "/home/{{ hadoop_user }}/tmp"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    mode: 0750

- name: Create Namenode directory
  file:
    path: "/home/{{ hadoop_user }}/hadoop-data/hdfs/namenode"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    mode: 0750

- name: Create Datanode directory
  file:
    path: "/home/{{ hadoop_user }}/hadoop-data/hdfs/datanode"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    mode: 0750

- name: Create logs directory
  file:
    path: "{{ hadoop_home }}/logs"
    state: directory
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    mode: 0750

- name: Add the service scripts
  template:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
  with_items:
    - {
        src: "core-site.xml",
        dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml",
      }
    - {
        src: "hdfs-site.xml",
        dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml",
      }
    - {
        src: "yarn-site.xml",
        dest: "{{ hadoop_home }}/etc/hadoop/yarn-site.xml",
      }
    - {
        src: "mapred-site.xml",
        dest: "{{ hadoop_home }}/etc/hadoop/mapred-site.xml",
      }
    - {
        src: "capacity-scheduler.xml",
        dest: "{{ hadoop_home }}/etc/hadoop/capacity-scheduler.xml",
      }

- name: Read Java home
  shell: dirname $(dirname $(readlink $(readlink $(which java))))
  register: java_home_reg

- name: Update java_home
  set_fact: "java_home={{ java_home_reg.stdout_lines[0] }}"

- name: Export JAVA_HOME within hadoop env
  lineinfile:
    dest: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
    regexp: "^export JAVA_HOME="
    line: "export JAVA_HOME={{ java_home }}"

- name: Add Azure libs to HADOOP_CLASSPATH
  lineinfile:
    dest: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
    regexp: "^export HADOOP_CLASSPATH="
    line: "export HADOOP_CLASSPATH={{ hadoop_home }}/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH"
    state: present

- name: Add Azure tools to hadoop-env.sh
  lineinfile:
    dest: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
    regexp: "^export HADOOP_OPTIONAL_TOOLS="
    line: "export HADOOP_OPTIONAL_TOOLS='hadoop-azure,hadoop-azure-datalake'"

- name: Set proper ownership
  file:
    path: "{{ hadoop_home }}/"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    recurse: yes
    mode: 0750

- name: Add hadoop and YARN services to systemd
  template:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
  with_items:
    - {
        src: "namenode.service.j2",
        dest: "/etc/systemd/system/namenode.service",
      }
    - {
        src: "secondarynamenode.service.j2",
        dest: "/etc/systemd/system/secondarynamenode.service",
      }
    - {
        src: "datanode.service.j2",
        dest: "/etc/systemd/system/datanode.service",
      }
    - {
        src: "nodemanager.service.j2",
        dest: "/etc/systemd/system/nodemanager.service",
      }
    - {
        src: "resourcemanager.service.j2",
        dest: "/etc/systemd/system/resourcemanager.service",
      }

- name: Reload systemd
  systemd:
    daemon_reload: yes

- name: Enable DataNode services to start at boot
  systemd:
    name: "{{ item.name }}"
    enabled: yes
    masked: no
  with_items:
    - { name: datanode }
    - { name: nodemanager }

- name: Clean-up temporary directory
  file:
    path: "{{ hadoop_temp }}"
    state: absent

- name: Drop successful install note
  file:
    path: "{{ stat_file }}"
    state: touch
